# -*- coding: utf-8 -*-
"""Homework Fabrizi Francesco 1883509

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xe_RlVNXkAbRpa8jtHLSUyshC9kMZWb5

### For the homeworks we are going to use the "[Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)"

The dataset can be used both for regression and classification tasks.

#### Source:

Kelwin Fernandes INESC TEC, Porto, Portugal/Universidade do Porto, Portugal.
Pedro Vinagre ALGORITMI Research Centre, Universidade do Minho, Portugal
Paulo Cortez ALGORITMI Research Centre, Universidade do Minho, Portugal
Pedro Sernadela Universidade de Aveiro

#### Data Set Information:

* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.
* Acquisition date: January 8, 2015
* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method. See their article for more details on how the relative performance values were set.

Attribute Information:

Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)

Attribute Information:
0. url: URL of the article (non-predictive)
1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)
2. n_tokens_title: Number of words in the title
3. n_tokens_content: Number of words in the content
4. n_unique_tokens: Rate of unique words in the content
5. n_non_stop_words: Rate of non-stop words in the content
6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content
7. num_hrefs: Number of links
8. num_self_hrefs: Number of links to other articles published by Mashable
9. num_imgs: Number of images
10. num_videos: Number of videos
11. average_token_length: Average length of the words in the content
12. num_keywords: Number of keywords in the metadata
13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?
14. data_channel_is_entertainment: Is data channel 'Entertainment'?
15. data_channel_is_bus: Is data channel 'Business'?
16. data_channel_is_socmed: Is data channel 'Social Media'?
17. data_channel_is_tech: Is data channel 'Tech'?
18. data_channel_is_world: Is data channel 'World'?
19. kw_min_min: Worst keyword (min. shares)
20. kw_max_min: Worst keyword (max. shares)
21. kw_avg_min: Worst keyword (avg. shares)
22. kw_min_max: Best keyword (min. shares)
23. kw_max_max: Best keyword (max. shares)
24. kw_avg_max: Best keyword (avg. shares)
25. kw_min_avg: Avg. keyword (min. shares)
26. kw_max_avg: Avg. keyword (max. shares)
27. kw_avg_avg: Avg. keyword (avg. shares)
28. self_reference_min_shares: Min. shares of referenced articles in Mashable
29. self_reference_max_shares: Max. shares of referenced articles in Mashable
30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable
31. weekday_is_monday: Was the article published on a Monday?
32. weekday_is_tuesday: Was the article published on a Tuesday?
33. weekday_is_wednesday: Was the article published on a Wednesday?
34. weekday_is_thursday: Was the article published on a Thursday?
35. weekday_is_friday: Was the article published on a Friday?
36. weekday_is_saturday: Was the article published on a Saturday?
37. weekday_is_sunday: Was the article published on a Sunday?
38. is_weekend: Was the article published on the weekend?
39. LDA_00: Closeness to LDA topic 0
40. LDA_01: Closeness to LDA topic 1
41. LDA_02: Closeness to LDA topic 2
42. LDA_03: Closeness to LDA topic 3
43. LDA_04: Closeness to LDA topic 4
44. global_subjectivity: Text subjectivity
45. global_sentiment_polarity: Text sentiment polarity
46. global_rate_positive_words: Rate of positive words in the content
47. global_rate_negative_words: Rate of negative words in the content
48. rate_positive_words: Rate of positive words among non-neutral tokens
49. rate_negative_words: Rate of negative words among non-neutral tokens
50. avg_positive_polarity: Avg. polarity of positive words
51. min_positive_polarity: Min. polarity of positive words
52. max_positive_polarity: Max. polarity of positive words
53. avg_negative_polarity: Avg. polarity of negative words
54. min_negative_polarity: Min. polarity of negative words
55. max_negative_polarity: Max. polarity of negative words
56. title_subjectivity: Title subjectivity
57. title_sentiment_polarity: Title polarity
58. abs_title_subjectivity: Absolute subjectivity level
59. abs_title_sentiment_polarity: Absolute polarity level
60. shares: Number of shares (target)


The first two columns (url and time_delta) are non-predictive and should be ignored

The last column **shares** contains the value to predict.

### Regression
In the case of regression we want to predict the value of the share column.

### Classification
In the case of classification we want to predict one of two classes:

* *low* -- shares < 1,400
* *high* -- shares >= 1,400

### Metrics

#### Regression
To evaluate how good we are doing on the **regression** task we will use the Root Mean Squared Error (RMSE). RMSE is given by

$$
\sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}{\Big(d_i -f_i\Big)^2}}
$$


where:

* $n$ is the number of test samples
* $d_i$ is the ground truth value of the i-th sample
* $f_i$ is the predicted value of the i-th sample


#### Classification
To evaluate how good we are doing on the **classification** task we will use the accuracy metrics. Accuracy is given by

$$
\frac{TP+TN}{TP+TN+FP+FN}
$$

where:

* TP is the number of *correctly* classified positive samples
* TN is the number of *correctly* classified negative samples
* FP is the number of *incorrectly* classified positive samples
* FN is the number of *incorrectly* classified negative samples
"""

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip

!unzip OnlineNewsPopularity.zip

import pandas as pd
import numpy as np
from collections import Counter
import math
import matplotlib.pyplot as plt

"""Format properly the names of the columns and remove the first two columns"""

df = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')
df = df.rename(columns=lambda x: x.strip())
df = df.iloc[: , 2:]

df.dtypes

"""## Let's plot some of the columns"""

import matplotlib.pyplot as plt
import seaborn as sns

columns_to_plot = [
    'num_videos',
    'num_imgs',
    'num_keywords',
    'data_channel_is_world',
    'rate_negative_words',
    'self_reference_avg_sharess',
]

fig, ax = plt.subplots(len(columns_to_plot), 1, figsize=(20, 20))

for i, column in enumerate(columns_to_plot, 0):
  ax[i].hist(df[column])
  ax[i].title.set_text(column)

plt.show()

"""# Utili

Importo delle librerie utili per il funzionamento dei vari codici
"""

import pandas as pd
import numpy as np
from collections import Counter
import math
import matplotlib.pyplot as plt

df = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')
df = df.rename(columns=lambda x: x.strip())
df = df.iloc[: , 2:]

"""l'ultima colonna contiene gli shares che sono i valori da predirre. Separiamo le feature dal target e per comodità inserisco una colonna che classifica le shares come true o false in base se sono >=1400 o no, questa colonna sarà utile per i problemi di classificazione mentre userò la colonna originale per quelli di regressione"""

df['higth'] = np.where(df['shares']>=1400, True, False)
df.higth

"""accuracy per verificare l'accuratezza nel caso di classificazione



"""

def accuracy(y_true, y_pred):
        accuracy = np.sum(y_true == y_pred) / len(y_true)
        return accuracy

"""Root Mean Squared Error (RMSE) per valutare la regressione"""

def rmse(y, y_h):
        error = 0
        for i in range(len(y)):
            diff=y[i]-y_h[i]
            error += (diff) ** 2
        return math.sqrt(error / len(y))

"""# Decision tree

Preparo i dati per usarli nel decision tree.
Nelle x metto le feature usando iloc[:,  :-2].values in questo modo escludo le ultime due colonne e considero solo i valori delle colonne prese da x.
La y prende solo l'ultima colonna iloc[:, -1] ovvero la colonna di True e False.
Poi usa sklearn per creare il train e il test in modo casuale
"""

x = df.iloc[:, :-2].values

y = df.iloc[:, -1].values

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=1234
)

"""Realizzo la classe Nodo per i miei alberi di decisione e nel caso di nodi leaf ovvero i nodi alla fine dell'albero ho un attributo di tipo value. Infatti nel caso il valore di value!=None allora è una foglia"""

class Node:
    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value

    def is_leaf_node(self):
        return self.value is not None

"""realizzo il vero e proprio albero di decisione.
* __init__(): è il costruttore, al suo interno imposto varie variabili tra cui le più importanti sono:
    1. max_depth ovvero la profondità massima dell'albero da anlizzare.
    2. min_samples_split il numero minimo di samples  necessari per dividere un nodo
    3. features non usa tutte le features a disposizione ma solo una selezione

*   build(x, y)la funzione costruisce in modo ricorsivo un albero decisionale in caso vengano rispettati i criteri di arresto si ferma la costruzione.
Usa la funzione bestsplit per ricevere la miglior combinazione (split, threshold).
Crea i nodi figli usando lo split() e poi costruisce l'albero ricorsivamente usando la funzione bulid().
Alla fine restituisce il nodo
*   fit(x, y) chiama la build() e memorizza l'albero costruito nel costruttore
*   mc_label(self, y) restituisce il valore del target più presente
*   bestsplit(self, x, y, feature)  calcola tutte le possibili combinazione  (rsplit, rthreshold) e restituisce la migliore usando information gain. Nel nostro caso selezioniamo le feature prendendole randomicamente dalle feature (n_f) disponibili per un numero pari a self.features
*  infgain(self, y, xc, threshold) calcola l'information gain di uno split.
Per prima cosa calcolo l'entropia del padre, usando la funzione split creo i due figli (destra e sinistra) se lo split è avvenuto con successo allora calcolo la media ponderata dell'entropia dei figli e la sottrae a quella del padre
* entropy(self, y) calcola l'entropia
*  split(self, x_column, thresh) divide gli elementi a seconda che siano > o <= del threshold (t). creando il ramo di destra e sinistra
* predict(self, x) per ogni  xx in x applica la funzione ttree(), e restituisce un array di tali risultati
* ttree(self, x, node) attraversa l'albero ricorsivamente per classificare una singola istanza. Controllo se è un nodo foglia in tal caso restituisco node.value.













"""

class DecisionTree:
    def __init__(self, min_samples_split=2, max_depth=100, features=None):
        self.min_samples_split=min_samples_split
        self.max_depth=max_depth
        self.features=features
        self.root=None

    def infgain(self, y, xc, threshold):
        parent_entropy = self.entropy(y)

        l, r = self.split(xc, threshold)
        if len(l) == 0 or len(r) == 0:
            return 0

        n_l=len(l)
        n_r=len(r)
        e_l=self.entropy(y[l])
        e_r=self.entropy(y[r])
        child_l=(n_l/len(y)) * e_l
        child_r=(n_r/len(y)) * e_r
        child_entropy =child_l+child_r

        ig = parent_entropy - child_entropy
        return ig

    def entropy(self, y):
        ricorrenze= np.bincount(y)
        pp=ricorrenze / len(y)
        return -np.sum([p * np.log(p) for p in pp if p>0])

    def fit(self, x, y):
        self.features = x.shape[1] if not self.features else min(x.shape[1],self.features)
        self.root = self.build(x, y)

    def mc_label(self, y):
        counter = Counter(y)
        value = counter.most_common(1)[0][0]
        return value

    def build(self, x, y, depth=0):
        n_samples, n_f = x.shape
        n_labels = len(np.unique(y))

        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):
            leaf_value =  self.mc_label(y)
            return Node(value=leaf_value)

        feature = np.random.choice(n_f, self.features, replace=False)

        bfeature, bthresh = self.bestsplit(x, y, feature)

        lefti, righti= self.split(x[:, bfeature], bthresh)
        left = self.build(x[lefti, :], y[lefti], depth+1)
        right = self.build(x[righti, :], y[righti], depth+1)
        return Node(bfeature, bthresh, left, right)


    def split(self, x_column, t):
        left= np.argwhere(x_column <= t).flatten()
        right= np.argwhere(x_column > t).flatten()
        return left, right

    def bestsplit(self, x, y, feature):
        best_gain = -1
        rsplit, rthreshold = None, None

        for f in feature:
            xc = x[:, f]
            thresholds = np.unique(xc)

            for t in thresholds:

                gain = self.infgain(y, xc, t)

                if gain > best_gain:
                    best_gain = gain
                    rsplit = f
                    rthreshold = t

        return rsplit, rthreshold



    def predict(self, X):
        return np.array([self.ttree(x, self.root) for x in X])

    def ttree(self, x, node):
        if node.is_leaf_node():
            return node.value

        if x[node.feature] <= node.threshold:
            return self.ttree(x, node.left)
        return self.ttree(x, node.right)

clf = DecisionTree(max_depth=10)
clf.fit(x_train, y_train)
predictions = clf.predict(x_test)

"""valuto l'accuracy del mio decision tree"""

print("Accuracy:", accuracy(y_test, predictions)*100)

"""confronto con sklearn"""

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(
    criterion="entropy"
)

model = model.fit(x_train,y_train)

y_pred = model.predict(x_test)

from sklearn import metrics

print("Accuracy:",metrics.accuracy_score(y_test, y_pred)*100)

"""# Linear Regression

**senza regolarizzazione**

realizzo il mylinearRegression senza regolarizzazione usando gradient descent.


* __init__(): contine il tasso di apprendimento, il numero di iterazioni oltre che ai pesi (weights) e al bias (w0). Si potrebbe inoltre inserire un elenco che memorizza il Loss, utlie se si volesse vedere la curva di apprendimento.
*   mse(): uso l'errore quadratico medio come funzione di costo da minimizzare e rappresenta l'errore. Con y i valori veri che mi aspetto e y_h i valori che ho trovato (predetto).
*  fit(): funzione che cerca di ottimizzare i valori dei pesi e del bias usando gradiant descnt, per prima cosa imposto il peso e il bias a 0.
Poi comincio ad usare il gradient descent e calcolo le derivate e aggiorno il peso e il bias
*   predict() funzione che predice usando quazioni lineari

Nel caso decidessi di memorizzare Loss=[ ] sarebbe una lista che memorizza i valori di loss durante la fase di addestramento e può essere utilizzata per analizzare il comportamento del modello
"""

class myLinearRegression:
    def __init__(self, learning_rate=0.01, iterations=100):
        self.learning_rate = learning_rate
        self.iterations=iterations
        self.w=None
        self.b=None

    @staticmethod
    def mse(y, y_h):
        error = 0
        for i in range(len(y)):
            diff=y[i]-y_h[i]
            error += (diff) ** 2
        return error / len(y)

    def fit(self, x, y):

        self.w= np.zeros(x.shape[1])
        self.b= 0

        for i in range(self.iterations):

            y_h= np.dot(x, self.w) + self.b


            partial_w = (1 / x.shape[0]) * (2 * np.dot(x.T, (y_h - y)))
            partial_d = (1 / x.shape[0]) * (2 * np.sum(y_h - y))

            self.w-= self.learning_rate * partial_w
            self.b-= self.learning_rate * partial_d


    def predict(self, x):
        return np.dot(x, self.w) + self.b

"""Uso strain_test_split come nel caso degli alberi di decisione ma ora la y contiene la penultima colonna e non la colonna dei True e False

**uso quasi sempre la normalizzazione sui dati poichè non facendolo avevo spesso errori dovuti a overflow**
"""

x = df.iloc[:, :-2].values
y = df.iloc[:, -2].values

import numpy as np

mean = np.mean(x, axis=0)
std = np.std(x, axis=0)

x1 = (x - mean) / std



from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x1, y, test_size=0.2, random_state=1234)

model = myLinearRegression()
model.fit(x_train, y_train)
preds = model.predict(x_test)

"""calcolo RMSE"""

rmse(y_test, preds)

"""confronto con LinearRegression di sklearn"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lr_model = LinearRegression()
lr_model.fit(x_train, y_train)
lr_preds = lr_model.predict(x_test)
print(math.sqrt(mean_squared_error(y_test, lr_preds)))

"""**Con regolarizzazione L1, L2 e Elastic Net**

In questo codice uso la regolarizzazione utile per evitare overfitting.
Il codice è molto simile a quello precendete senza regolarizzazione.
La mia funzione di costo da minimizzare non sarà solo data da EmpLoss ovvero mse ma anche da una termine che mi misura la complessità del mio modello, la somma tra questi due valori mi darà il costo.

$$ Cost(h)= EmpLoss(h) + λComplexity(h) $$

Uso una regolarizzazione L1, L2 e Elastic Net.
Con L1 la complessità sarà la somma in modulo dei pesi:

$$
\sum\limits_{i=1}^{n}{\Big|w_i\Big|}
$$

con L2 sarà la somma dei quadrati dei pesi:

$$
\sum\limits_{i=1}^{n}{\Big|w_i\Big|^2}
$$

con Elastic Net è la combinazione di L1 e L2

$$ λ_1L_1(w) + λ_2L_2(w) $$
"""

class myRLinearRegression:
    def __init__(self, learning_rate=0.01, iterations=1000, regular="None",lamb=1):
        self.learning_rate = learning_rate
        self.iterations=iterations
        self.w=None
        self.b=None
        self.loss = []
        self.lamb=lamb
        self.regular=regular

    @staticmethod
    def costo(y, y_h, self):
        error = 0
        for i in range(len(y)):
            diff=y[i]-y_h[i]
            error += (diff) ** 2
        mse=error/len(y)
        if(self.regular=="L1" or self.regular=="None" ):
          compl=np.sum(np.abs(self.w))*self.lamb
        if(self.regular=="L2"):
          compl=np.sum(self.w**2)*self.lamb
        if(self.regular=="EN"):
          a=self.lamb/(self.lamb+self.lamb)
          compl=np.sum(np.abs(self.w))*a
          compl+=np.sum(self.w**2)*(1-a)
        return mse+compl

    def fit(self, x, y):

        self.w= np.zeros(x.shape[1])
        self.b= 0

        for i in range(self.iterations):

            y_h= np.dot(x, self.w) + self.b
            loss = self.costo(y, y_h, self)
            self.loss.append(loss)

            if self.regular == "L1" or self.regular is None:
                reg = np.sign(self.w) * self.lamb
            elif self.regular == "L2":
                reg = 2 * self.w * self.lamb
            elif self.regular == "EN":
                a = self.lamb / (self.lamb + self.lamb)
                reg = a * np.sign(self.w) + (1 - a) * 2 * self.w

            partial_w = ((1 / x.shape[0]) * (2 * np.dot(x.T, (y_h - y)))) + reg
            partial_d = (1 / x.shape[0]) * (2 * np.sum(y_h - y))

            self.w -= self.learning_rate * partial_w
            self.b -= self.learning_rate * partial_d

    def predict(self, x):
        return np.dot(x, self.w) + self.b

model2=myRLinearRegression(regular="L2",lamb=0.001)
model2.fit(x_train, y_train)
preds2 = model2.predict(x_test)
model1=myRLinearRegression(regular="L1",lamb=0.00000001)
model1.fit(x_train, y_train)
preds1 = model1.predict(x_test)
model3=myRLinearRegression(regular="EN",lamb=0.01)
model3.fit(x_train, y_train)
preds3 = model3.predict(x_test)

def rmse(y, y_h):
        error = 0
        for i in range(len(y)):
            diff=y[i]-y_h[i]
            error += (diff) ** 2
        return math.sqrt(error / len(y))

print("rmse di L1:", rmse(y_test, preds1))

print("rmse di L2:", rmse(y_test, preds2))

print("rmse di EN:", rmse(y_test, preds3))

print("rmse senza regolarizzazione:",rmse(y_test, preds))

"""Uso sklearn per fare un paragone"""

from sklearn.linear_model import Lasso
import numpy as np

lasso_reg = Lasso(alpha=0.001)
lasso_reg.fit(x_train, y_train)
y_pred1 = lasso_reg.predict(x_test)

from sklearn.linear_model import Ridge

ridge_reg = Ridge(alpha=0.001)
ridge_reg.fit(x_train, y_train)
y_pred2 = ridge_reg.predict(x_test)

from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.01, l1_ratio=1)
elastic_net.fit(x_train, y_train)
y_pred3 = elastic_net.predict(x_test)

print("rmse di L1 (sklearn):", rmse(y_test, y_pred1))

print("rmse di L2 (sklearn):", rmse(y_test, y_pred2))

print("rmse di EN (sklearn):", rmse(y_test, y_pred3))

print("rmse senza regolarizzazione (sklearn) :",rmse(y_test, lr_preds))

"""# Linear Classification as Regression

l'ultima colonna continete shares che è il valore da predirre. Separiamo le feature dal target e per comodità uso per le y la colonna che ho aggiunto che ha valori True e False
"""

df['higth'] = np.where(df['shares']>=1400, True, False)
df.higth

x = df.iloc[:, :-2].values
y = df.iloc[:, -1].values

import numpy as np

mean = np.mean(x, axis=0)
std = np.std(x, axis=0)

x1 = (x - mean) / std


from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x1, y, test_size=0.2, random_state=1234
)

"""Nel caso di classificazione usiamo la Perceptron che utilizza la classificazione binaria. L'algoritmo si addestra iterativamente con i dati di addestramento e aggiornare i pesi e il bias.


* __init__(): inizializza l'oggetto impostando w (peso) e b (bias) a None e i parametri learning_rate e iterazioni
* fit(): addestra il Perceptron sui dati x. Con x.shape salvo il numero di samples e di features.
Inizializzo w e b a 0.
yy è una "copia" di y in un formato binario usando np.where (se necessario).
Poi attraverso i cicli va ad aggiornare i pesi e il bias e usando la funzione di Threshold vista a lezione in cui h(x)=1 se >=0 altrimenti è 0.
Da questa  aggiorno:
$$w=w+learning_rate*(yy[id]-h(x))*xi$$ a cui ho aggiunto il bias per averre un accuracy migliore.
* predict(): utilizza il Perceptron addestrato in precedenza con la funzione fit() per classificare nuovi esempi x.
* funzione() è la funzione a gradino che restituisce 1 se l'input è >=0 altrimenti restituisce 0


"""

class Perceptron:

    def __init__(self, learning_rate=0.01, iterazioni=1000):
        self.learning_rate = learning_rate
        self.iterazioni=iterazioni
        self.w=None
        self.b=None


    def fit(self, x, y):
        n_samples, n_features = x.shape
        self.w=np.zeros(n_features)
        self.b=0

        yy= np.where(y > 0 , 1, 0)

        for _ in range(self.iterazioni):
            for id, xi in enumerate(x):
                lout = np.dot(xi, self.w) + self.b
                y_p=self.funzione(lout)

                u=self.learning_rate * (yy[id] - y_p)
                self.w+= u*xi
                self.b+= u


    def predict(self, x):
        lout = np.dot(x, self.w) + self.b
        y_p= self.funzione(lout)
        print(y_p)
        return y_p

    def funzione(self, x):
        return np.where(x >= 0, 1, 0)

p = Perceptron(learning_rate=0.01, iterazioni=1000)
p.fit(x_train, y_train)
predictions = p.predict(x_test)

"""definisco accuracy e stampo il risultato"""

def accuracy(y_true, y_pred):
        accuracy = np.sum(y_true == y_pred) / len(y_true)
        return accuracy

print(accuracy(y_test, predictions)*100)

"""Uso sklearn per verificare"""

from sklearn.linear_model import Perceptron
perceptron = Perceptron()
perceptron.fit(x_train, y_train)
pperceptron = perceptron.predict(x_test)

print(accuracy(y_test, pperceptron)*100)

"""# Classificazione con regressione logistica

implemento un modello di regressione logistica che è un algoritmo di apprendimento automatico per la classificazione binaria. L'algoritmo utilizza una funzione sigmoide per calcolare la probabilità che un campione appartenga alla classe positiva, e  utilizza il metodo del gradient descent per minimizzare la funzione di perdita e per aggioranre i valore di peso (w) e bias (b).

* __init__(): è il costruttore della classe:
    1. learning_rate rappresenta il tasso di apprendimento per l'algoritmo di discesa del gradiente.
    2. iterazioni rappresenta il numero di iterazioni da eseguire
    3. w e b sono rispettivamente i pesi e il bias che verranno aggiornati durante l'addestramento.

* sigmoid(): funzione sigmoide utilizzata per trasformare la somma pesata degli input in una probabilità tra 0 e 1:
              
$$\frac{1}{1+e^{-z}}$$

            

*   BCE():calcola la funzione di costo della regressione logistica, ovvero la binary cross-entropy loss:
$$-(y*log(h))+(1-y)*log(1-h))$$

*   fit(): metodo principale che allena il modello usando i dati di x.
inizializza i parametri w e b, poi ottimizza in modo iterativo pesi e bias attraverso gradient descent, calcola le derivate e aggiorna w e b
* pprob() calcola le probabilità di appartenenza alla classe positiva utilizzando le equazioni lineare passate alla funzione  sigmoidea
* predict() esegue la previsione sui dati di input
"""

class myLogisticRegression:

    def __init__(self, learning_rate=0.01, iterazioni=1000):
        self.learning_rate = learning_rate
        self.iterazioni=iterazioni
        self.w=None
        self.b=None

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def BCE(self, y, h):
        t= 0
        for yy, hh in zip(y, h):
            t += (yy * np.log(hh)) + (1 - yy) *np.log(1-hh)
        return - t / len(y)

    def fit(self, x, y):

        self.w = np.zeros(x.shape[1])
        self.b = 0

        for i in range(self.iterazioni):
            lp = np.dot(x, self.w) + self.b
            h = self.sigmoid(lp)


            pw = (1 / x.shape[0]) * (np.dot(x.T, (h - y)))
            pd = (1 / x.shape[0]) * (np.sum(h - y))


            self.w -= self.learning_rate * pw
            self.b -= self.learning_rate * pd

    def pprob(self, x):
        pred=np.dot(x, self.w) + self.b
        return self.sigmoid(pred)

    def predict(self, x, threshold=0.5):
        probab=self.pprob(x)
        return [1 if i > threshold else 0 for i in probab]

"""ho normalizzato i dati per evitare problemi con il calcolo del sigmoid"""

df['higth'] = np.where(df['shares']>=1400, True, False)
df.higth

x = df.iloc[:, :-2].values
y = df.iloc[:, -1].values

import numpy as np

mean = np.mean(x, axis=0)
std = np.std(x, axis=0)

x1 = (x - mean) / std




from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x1, y, test_size=0.2, random_state=1234
)

model = myLogisticRegression()
model.fit(x_train, y_train)
predsss = model.predict(x_test)

"""calcolo l'accuratezza"""

print(accuracy(y_test, predsss)*100)

"""faccio un confronto con sklearn"""

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression()
lr_model.fit(x_train, y_train)
lr_preds = lr_model.predict(x_test)

print(accuracy(y_test, lr_preds)*100)

"""# Classificazione con algoritmo di k-Nearest Neighbors

K-Nearest Neighbors (KNN) non richiede apprendimento, ma calcola semplicemente la distanza, in questo caso usando la distanza euclidea. Si basa sul principio che gli oggetti con caratteristiche simili tendono ad essere vicini l'uno all'altro nello spazio delle caratteristiche. Per assegnare un determinato output vedo l'etichetta più comune tra i k vicini e assegno quella all'output dei miei dati di test.
Anche in questo caso ho normalizzato i dati x.


* __init__(k): memorizza il nuemro di vicini (k), ho impostato come valore predefinito k=5.
* disteuclidea(): è la funzione che calcola la distanza euclidea tra i punti p e q:

$$ \sqrt{\sum\limits_{i=1}^{n}{\Big(p_i -q_i\Big)^2}} $$

* fit(): prende in input dei dati x e dei target y e li memorizza in xt e yt
* predict(): per ogni dato in x calcola la distanza euclidea tra tale punto e tutti i punti in xt. Poi seleziona i k punti (dei xt) con la distanza euclidea minore e usa i loro target (contenuti in yt) per determinare il valore di output del valore del dato di x in esame.
Usando np.bincount conta il numero di occorrenze e restituisce il valore più comune.
Tutte le predizioni vengono restituite come un array
"""

class KNN:
    def __init__(self, k=5):
        self.k = k

    def disteuclidea(self, p, q):
        return np.sqrt(np.sum((p - q) ** 2))

    def fit(self, x, y):

        self.xt = x
        self.yt = y

    def predict(self, x):
        predictions = []
        for i in x:
            d= [self.disteuclidea(i, q) for q in self.xt]
            indicivicini = np.argsort(d)[:self.k]
            valorivicini = self.yt[indicivicini]
            mcomune= np.bincount(valorivicini).argmax()
            predictions.append(mcomune)

        return np.array(predictions)

df['higth'] = np.where(df['shares']>=1400, True, False)
df.higth

x = df.iloc[:, :-2].values
y = df.iloc[:, -1].values

import numpy as np

mean = np.mean(x, axis=0)
std = np.std(x, axis=0)

x1 = (x - mean) / std




from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x1, y, test_size=0.2, random_state=1234
)

model = KNN()
model.fit(x_train, y_train)
preds = model.predict(x_test)

"""vedo l'accuratezza del mio risultato con k=5. Sarebbe possibile provarla con altri valori di k (preferibbilmente dispari) in modo da trovare il valore di k che mi restituisce un accuracy migliore. Nel mio caso uso k=5 che comunque mi restituisce un valore buono"""

print(accuracy(y_test, preds)*100)

"""uso sklearn per verificare se il mio codice funziona. In questo caso faccio usare diversi valori di k per vedere come possono cambiare i risultati. Con k=5 i risultati sono simili"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
for k in range(1, 16, 2):
    knn_model = KNeighborsClassifier(n_neighbors=k)
    knn_model.fit(x_train, y_train)
    knn_preds = knn_model.predict(x_test)
    print("accuracy con k=", k,accuracy(y_test, knn_preds)*100)

"""# Regressione con algoritmo di k-Nearest Neighbors

identico al precedente solo che invece che considerare il valore più presente dei vicini, faccio una media dei valori vicini
"""

class KNNR:
    def __init__(self, k=5):
        self.k = k

    def disteuclidea(self, p, q):
        return np.sqrt(np.sum((p - q) ** 2))

    def fit(self, x, y):

        self.xt = x
        self.yt = y

    def predict(self, x):
        predictions = []
        for i in x:
            d= [self.disteuclidea(i, q) for q in self.xt]
            indicivicini = np.argsort(d)[:self.k]
            valorivicini = self.yt[indicivicini]
            predictions.append(np.mean(valorivicini))

        return np.array(predictions)

x = df.iloc[:, :-2].values
y = df.iloc[:, -2].values

import numpy as np

mean = np.mean(x, axis=0)
std = np.std(x, axis=0)

x1 = (x - mean) / std




from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x1, y, test_size=0.2, random_state=1234
)

"""Vado a testare la mia funzione KNNR e la valuto calcolando rmse. Anche in questo caso io uso K=5"""

modelr = KNNR()
modelr.fit(x_train, y_train)
predsr = modelr.predict(x_test)

print(rmse(y_test,predsr))

"""uso la funzione sklearn per calcolare k-Nearest Neighbors provando con diversi valori di k e valutandoli calcolando il rmse"""

from sklearn.neighbors import KNeighborsRegressor

for k in range(1, 16, 2):
    knnr_model = KNeighborsRegressor(n_neighbors=k)
    knnr_model.fit(x_train, y_train)
    knnr_preds = knnr_model.predict(x_test)
    print("rmse con k=", k,rmse(y_test, knnr_preds))

"""#Regressione con Reti Neurali

Realizzo una rete neurale per la regressioe a tre strati: un input layer, un hidden layer e un output layer in cui posso variare il numero di elementi nel hiddenn layer.
* __init__(): è il costruttore che prende in input i_s (input size), hs (hidden size) e os (output size), che rappresentano rispettivamente il numero di neuroni nell'input layer, nel hidden layer e nell'output layer. Inizializzo casualmente i pesi e il bias tra lo strato di input e lo strato nascosto wh1 e bh1 e i   pesi e il bias tra lo strato nascosto e lo strato di output wo2 e bo2.
* relu()  implementa la funzione di attivazione ReLU:
$$max(0,x)$$

* relu_deriv() la derivata della funzione relu
* mse(): La funzione di costo utilizzata è il Mean Squared Error (MSE):


$$
\frac{1}{n}\sum\limits_{i=1}^{n}{\Big(d_i -f_i\Big)^2}
$$

* mse_deriv() la derivata della funzione costo rispetto all'output del modello

* forward():implementa il forward pass, è responsabile di eseguire il passaggio in avanti della rete neurale. Riceve in input i dati di input x e calcola l'output predetto y_hat.
Inizia calcolando l'uscita del hidden layer (l1) e poi applico la funzione di attivazione relu e la salvo in a1 che sarà l'input del layer output.
Calcolo in fine l'output (y_hat) in maniera  lineare (moltiplicando l'attivazione dell'hidden layer a1 con i pesi dell'output layer wo2, cui viene sommato il bias dell'output layer bo2)

* backward(): implementa il backpropagation, calcola la retropropagazione dell'errore attraverso la rete neurale e aggiorna i pesi dei suoi parametri in base all'errore calcolato. Dopo la fase di  forward pass si procede con il backpropagation:
    1. Per prima cosa si calcola la derivata parziale dell'errore rispetto all'output usando la funzione di costo (in questo caso MSE).
Viene poi calcolato il gradiente dell'errore rispetto al secondo layer (quello che collega hidden a output). In fine si calcolano le derivate parziali per wo2 e bo2
    2. Viene calcolato il gradiente dell'errore rispetto all'hidden layer usando la derivata della funzione di attivazione ReLu (relu_deriv()) e salvato in l1_deriv. In fine si calcolano le derivate parziali per wh1 e bh1.

    3. alla fine vengono aggioranti i pesi e il bias usando il gradiant descent
  
* fit(): viene addestrata la rete neurale attraverso un loop di epochs epoche. Ad ogni loop eseguo forward propagation e poi backward propagation dove si aggiornano i pesi e il bias.
Ho messo una stampa ogni 500 epoche per vedere come scnedeva la loss.

* predict(): prendice il risultato dei dati di input x.

Ho usato y = y.reshape(-1, 1) perchè spesso avevo problemi di dimensioni.
"""

class NNR:
    def __init__(self, i_s, hs, os):
        self.wh1 = np.random.randn(i_s, hs) - 0.5
        self.bh1 = np.zeros((1, hs)) - 0.5
        self.wo2 = np.random.randn(hs, os) - 0.5
        self.bo2 = np.zeros((1, os)) - 0.5

    def relu(self, x):
        return np.maximum(0, x)

    def relu_deriv(self, a):
        return np.where(a > 0, 1, 0)

    def forward(self, x):
        self.l1 = np.dot(x, self.wh1) + self.bh1
        self.a1 = self.relu(self.l1)
        self.l2 = np.dot(self.a1, self.wo2) + self.bo2
        self.y_hat = self.l2

    def mse(self, y, y_hat):
        return np.mean(np.square(y - y_hat))

    def mse_deriv(self, y, y_hat):
        return (y_hat - y)

    def backward(self, x, y, lr):
        y = y.reshape(-1, 1)

        mse_deriv = self.mse_deriv(y, self.y_hat)
        l2_deriv = mse_deriv
        wo2_deriv = np.dot(self.a1.T, l2_deriv)
        bo2_deriv = np.sum(l2_deriv, axis=0, keepdims=True)

        a1_deriv = np.dot(l2_deriv, self.wo2.T)
        l1_deriv = a1_deriv * self.relu_deriv(self.l1)
        wh1_deriv = np.dot(x.T, l1_deriv)
        bh1_deriv = np.sum(l1_deriv, axis=0, keepdims=True)


        self.wh1 -= lr * wh1_deriv
        self.bh1 -= lr * bh1_deriv
        self.wo2 -= lr * wo2_deriv
        self.bo2 -= lr * bo2_deriv

    def train(self, x, y, epochs, lr):
        for i in range(epochs):
            y = y.reshape(-1, 1)
            self.forward(x)
            loss = self.mse(y, self.y_hat)
            self.backward(x, y, lr)
            if i % 500 == 0:
                print(f"Epoch {i}, Loss: {loss:.4f}")

    def predict(self, x):
        self.forward(x)
        return self.y_hat

"""
definisco i dati **solo in questo caso ho deciso di normalizzare anche le y invce che solo le x. questo perchè con dati y cosi grandi avevo numerosi problemi tra cui quelli di overflow**


**N.B il problema è stato testato anche con y non normalizzate, funziona comunque ma con risultati spesso non ottimali**"""

df['higth'] = np.where(df['shares']>=1400, True, False)
df.higth

x = df.iloc[: , :-2].values
y = df.iloc[: , -2].values

import numpy as np

mean = np.mean(x, axis=0)
std = np.std(x, axis=0)+ 1e-8

x1 = (x - mean) / std


meany = np.mean(y, axis=0)
stdy = np.std(y, axis=0)+ 1e-8

y1 = (y - meany) / stdy




from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x1, y1, test_size=0.2, random_state=1234
)

"""alleno la mia funzione e faccio la predizione"""

nett = NNR(x_train.shape[1], 6, 1)
nett.train(x_train, y_train, lr=0.0000001, epochs=5000)
predizioner = nett.predict(x_test)

"""rmse sulla mia predizione"""

print(rmse(y_test,predizioner))

"""faccio un paragone usando sklearn"""

from sklearn.neural_network import MLPRegressor
from sklearn.datasets import make_regression


modelr = MLPRegressor(4, activation='relu', max_iter=5000)
modelr.fit(x_train, y_train)
y_predrr = modelr.predict(x_test)
print(rmse(y_test,y_predrr))

"""# Classificazione con Reti Neurali

Molto simile a quella realizzata precedentemente per la regressione, è un codice per la classificazione binaria, dove l'output può assumere solo due valori: 0 o 1.
La rete neurale ha un singolo strato nascosto e utilizza la funzione di attivazione ReLU per il primo strato e la funzione di attivazione sigmoide per l'output.

* __init__(): identico al precendete dove inizializzo randomicamnete wh1, wo2, bh1 e bo2.

* __relu()__ e relu_derivative(): come nel caso precedente

* sigmoid(): implementano la funzione di attivazione sigmoide:
$$\frac{1}{1+e^{-z}}$$

* sigmoid_derivative(): calcola la derivata della sigmoide:
$$ sigmoid(x)(1 - sigmoid(x)) $$

* forward() come prima implementa il Forward Pass calcolando l'output y_h.
Per prima cosa calcolo l1 e poi vine passto alla funzione relu che applica quindi la funzione di attivazione ReLu e restituisce a1.
a1 sarebbe l'input per la parte successiva per il calcolo di l2. l2 viene poi passato alla funzione di attivazione sigmoide e restituisce y_h

* backward():  implementa il passaggio all'indietro della rete neurale come nella NNR.

    1. Viene calcolato l'errore dell'ultimo strato, ovvero la differenza tra l'output predetto y_hat e l'etichetta y, moltiplicata per la derivata della funzione di attivazione sigmoid_derivative() applicata all'ultimo strato self.l2. Viene poi calcolato il gradiente per wo2 e bo2.

    2. Viene calcolato l'errore del livello nascosto, utilizzando il prodotto tra l'errore dell'ultimo strato errout e la matrice dei pesi dell'ultimo strato self.wo2 trasposta, moltiplicato per la derivata della funzione di attivazione relu_derivative() applicata al livello nascosto self.l1. Viene poi calcolato il gradiente per wh1 e bo1

    3. Vengono aggiornati i pesi e i bias


* fit(): esegue il processo di addestramento della rete neurale per il numero specificato di epoche, stampando l'errore di addestramento ogni 100 epoche.

* predict():  prende in input un insieme di dati di test e restituisce l'output. L'output viene arrotondato alla classe più probabile (0 o 1) utilizzando la funzione np.round().
"""

import numpy as np

class NNC:
    def __init__(self, i_s, hs, os):
        self.wh1 = np.random.rand(i_s, hs) - 0.5
        self.bh1 = np.random.rand(1, hs) - 0.5
        self.wo2 = np.random.rand(hs, os) - 0.5
        self.bo2 = np.random.rand(1, os) - 0.5


    def relu(self, a):
        return np.maximum(0, a)

    def relu_derivative(self, a):
        return np.where(a > 0, 1, 0)

    def sigmoid(self, a):
        return 1 / (1 + np.exp(-a)/10000)


    def sigmoid_derivative(self, a):
        return self.sigmoid(a) * (1 - self.sigmoid(a))


    def forward(self, x):
        self.l1 = np.dot(x, self.wh1) + self.bh1
        self.a1 = self.relu(self.l1)
        self.l2 = np.dot(self.a1, self.wo2) + self.bo2
        y_h = self.sigmoid(self.l2)

        return y_h

    def backward(self, x, y, y_hat, lr):
        y = y.reshape(-1, 1)
        errout = (y_hat - y) * self.sigmoid_derivative(self.l2)
        dw2 = np.dot(self.a1.T, errout)
        db2 = np.sum(errout, axis=0, keepdims=True)

        errhide = np.dot(errout, self.wo2.T) * self.relu_derivative(self.l1)
        dw1 = np.dot(x.T, errhide)
        db1 = np.sum(errhide, axis=0, keepdims=True)

        self.wh1 -= lr * dw1
        self.bh1 -= lr * db1
        self.wo2 -= lr * dw2
        self.bo2 -= lr * db2

    def fit(self, x, y, lr=0.1, epochs=1000):
        for i in range(epochs):
            y_h = self.forward(x)

            self.backward(x, y, y_h, lr)

            y = y.reshape(-1, 1)

            costo = np.mean(np.square(y_h - y))
            if i % 500 == 0:
                print(f"Epoch {i} - Loss: {costo}")


    def predict(self, X):
        y_h = self.forward(X)
        a=np.round(y_h)
        return a

"""preparo i dati"""

df['higth'] = np.where(df['shares']>=1400, True, False)
df.higth

x = df.iloc[:, :-2].values
y = df.iloc[:, -1].values

import numpy as np

mean = np.mean(x, axis=0)
std = np.std(x, axis=0)+ 1e-8

x1 = (x - mean) / std


from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x1, y, test_size=0.2, random_state=42
)

"""verifico il mio codice."""

net = NNC(x_train.shape[1], 6, 2)

net.fit(x_train, y_train, lr=0.1, epochs=5000)
predizione = net.predict(x_test)

"""verifico l'accuratezza"""

predizione =np.max(predizione, axis=1)
print(accuracy(y_test, predizione)*100)

"""verifico anche con sklearn"""

from sklearn.neural_network import MLPClassifier

clfc = MLPClassifier(6, max_iter=1000, random_state=42)

clfc.fit(x_train, y_train)

predicted_class = clfc.predict(x_test)

print(accuracy(y_test, predicted_class)*100)

"""# Resoconto

Ora descrivo brevemente i modelli sviluppati, alcuni accorgimenti che ho adottato e i risultati ottenuti. **Ho però descritto nel dettaglio ogni codice con la logica dietro il codice e le varie funzioni usate nel dettaglio in ogni sezione prima di scrivere il codice**

**Classificazione:** Per la classificazione sono stati usati vari metodi tra cui Decision Trees (sviluppato come una classificazione), Classificazione con regressione lineare, Classificazione con regressione logistica, Classificazione con algoritmo di k-Nearest Neighbors e Classificazione con Reti Neurali.
Tutti i codici sono stati valutati tramite una funzione accuracy:

$$
\frac{TP+TN}{TP+TN+FP+FN}
$$

e poi paragonati al codice preso dalla libreria sklearn. In questi casi (tranne negli alberi dove non avevo problemi di overflow e non avevo miglioramenti di accuracy) ho deciso di normalizzare i dati delle feature e solo dopo dividerli in test e train.
In questo modo sono riuscito a migliorare la mia accuracy e evitare problemi di overflow specialmente nei casi in cui veniva usata la funzione sigmoide ed inoltre è un processo molto importante per garantire un'adeguata preparazione dei dati prima dell'addestramento del modello migliorando le sue performance e migliorando la generalizzazione.
Per quanto riguarda i livelli di accuratezza sono tutti maggiori del 52%.
Ho avuto risultati migliori e stabili (cambiando spesso i dati di test e train), con la classificazione con regressione logistica dove ho ottenuto un accuracy sul 63% e anche con k-Nearest Neighbors sempre superiore al 60% (testata con k=5) ma impiega molto più tempo ad arrivare al risultato rispetto agli altri metodi. Per utlimo ho provato a realizzare la classificazione tramite Reti Neurali usando una rete neurale a tre strati (uno solo hidden) con due funzioni di attivazione (uno tra il primo e secondo strato e uno tra il secondo e il terzo) rispettivamente la funzione ReLu e sigmoide.
In questi casi l'accuracy supera sempre il 52% ma i risultati dipendono molto dai dati di test e train ma arriva spesso sopra il 60%, l'unico problema del codice è un errore: *RuntimeWarning: overflow encountered* nel calcolo della sigmoide che però non interferisce nel portare a termine il codice.
Resoconto risultati accuracy (mio codice e sklearn)

* Alberi 63%  58%                                  
* Linear Classification 56%  56%
* Con regressione logistica 66% 65%
* K-NN  61% 61%
* Reti neurali 52/65% 66%

**Regressione:** nel caso della regressione ho implementato i seguenti modelli:
Regressione lineare (senza regolarizzazione, regolarizzazione L1, L2, e elastic net)
Regressione con algoritmo di k-Nearest Neighbors e Regressione con Reti Neurali.
Sono stati tutti valutati tramite il calcolo del RMSE:

$$
\sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}{\Big(d_i -f_i\Big)^2}}
$$

Anche nel  caso della regressione ho normalizzato le x ma nel caso delle reti neurlai per la regressione ho deciso dopo vari tentativi di normalizzare anche le y.
Nel caso della regressione con le reti neurali ho che  la rmse esce abbastanza stabile con valori prossimi a 0.68 (il valore cosi basso è dovuto al fatto che anche i valori di y_train sono normalizzati), prima di normalizzare anche le y avevo provato ad usare due hidden layer poichè i risultati variavano molto in base ai dati di test ma non migliorava molto poichè dovevo inserire un learning rate molto basso per evitare overflow, ma normalizzando y ho risolto tutti i problemi.
Per K-Nearest Neighbors ho ottenuto un valore di rmse di 8800 circa simile a quello ottenuto usando sklearn con k=5 ma anche qui i tempi sono un po' più lunghi rispetto agli altri modelli.
Nel caso di Linear Regression ho scritto due codici uno senza regolarizzazione che ha ottenuto un rmse di circa 7700 (simile a quello di sklear). Per evitare overfitting (il modello non generalizza bene su nuovi dati non visti) si usa la regolarizzazione nel mio caso ho scritto un solo codice in cui poi è possibile decidere il tipo di regolarizzazione tra L1, L2 e elastic net i risultati di tali modelli restano comunque coerenti con quelli che ho ottenuto senza regolarizzazione.
Resoconto risultati RMSE (mio codice e sklearn)
* Linear Regression:
    1. Senza Regolarizzazione 7664,2 - 7643
    2. L1 7644,36 - 7644,8
    3. L2 7644,37 - 7643,5
    4. Elastic Net 7662 - 7644,8

* K-NN 8863  - 8863
* Reti neurali 0,67 - 0,66
"""